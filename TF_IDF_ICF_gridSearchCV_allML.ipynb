{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textvec import vectorizers\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "import glob, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report   \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "vocab = 8000\n",
    "\n",
    "\n",
    "data = pd.read_csv('/home/oem/jin/python/after_prepro10yr.csv', encoding=\"CP949\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:,1].values\n",
    "y = data.iloc[:,2].values\n",
    "\n",
    "\n",
    "tficf_vec = vectorizers.TfIcfVectorizer(sublinear_tf=True)\n",
    "\n",
    "\n",
    "\n",
    "count_vec = CountVectorizer(max_features=vocab,\n",
    "    token_pattern=r'\\w{1,}').fit(X)\n",
    "tfnt_vect = TfidfVectorizer(max_features=vocab).fit(X)\n",
    "\n",
    "tficf_vec.fit(count_vec.transform(X), y)\n",
    "icf = tficf_vec.transform(count_vec.transform(X))\n",
    "\n",
    "to_icf_vect = icf.toarray()\n",
    "tol_tf_vect = tfnt_vect.transform(X).toarray()\n",
    "tol_cout_vect = count_vec.transform(X).toarray()\n",
    "\n",
    "print(tol_tf_vect[1])\n",
    "\n",
    "ifidficf = []\n",
    "\n",
    "\n",
    "for line in range(0, len(to_icf_vect)):\n",
    "    lin_va = []\n",
    "    #print('totla',line)\n",
    "    for wo_val in range(0, vocab):\n",
    "        \n",
    "        tficf = tol_tf_vect[line][wo_val]*to_icf_vect[line][wo_val]/tol_cout_vect[line][wo_val] if tol_cout_vect[line][wo_val]!= 0 else 0\n",
    "        lin_va.append(tficf)\n",
    "    ifidficf.append(lin_va)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "X_totl = pad_sequences(ifidficf,dtype='float32',padding='post')\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_totl, y, test_size=0.25, shuffle=True, random_state=25, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### deault ML capplication #######\n",
    "\n",
    "\n",
    "svc = SVC()\n",
    "LogReg = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "rf = RandomForestClassifier()\n",
    "xgb = XGBClassifier()\n",
    "model = [svc, LogReg, nb, rf, xgb]\n",
    "\n",
    "\n",
    "for m in model:\n",
    "    m.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "for m in model:\n",
    "    pred = m.predict(X_test)\n",
    "    print('Acc for model', accuracy_score(y_test,pred))\n",
    "    print('Confusion Matrix for model', confusion_matrix(y_test, pred))\n",
    "    print('F1-score for model', classification_report(y_test, pred))\n",
    "    \n",
    "##### end of deault ML capplication #######    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### gridsearchCV for SVM classifer \n",
    "\n",
    "\n",
    "svm_tf_idf_icf = open('tf_idf_icf_svm.txt', 'w')\n",
    "\n",
    "parameters = [{\"kernel\":[\"rbf\"], \"gamma\":[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05,0.1, 1, 5, 10, 20, 50, 100, 200, 500, 1000], \"C\":[0.001, 0.005, 0.01, 0.1, 1, 5, 10, 20, 30, 40, 50, 100, 200, 500, 700, 1000], \"degree\":[1, 2, 3, 4, 5]}]\n",
    "\n",
    "#scores = [\"precision\", \"recall_macro\"]\n",
    "scores = [\"accuracy\"]\n",
    "\n",
    "print(\"# tuning hypter parmeter for %s\" % score)\n",
    "print()\n",
    "    \n",
    "clf_svm = GridSearchCV(SVC(random_state = 2), parameters, scoring=scores, verbose = 3, n_jobs=6, cv=5)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "    \n",
    "print(\"best parameters st found on devleopment set:\")\n",
    "print()\n",
    "print(clf_svm.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "means = clf_svm.cv_results_[\"mean_test_score\"]\n",
    "stds = clf_svm.cv_results_[\"std_test_score\"]\n",
    "    \n",
    "\n",
    "for mean, std, params in tqdm(zip(means, stds, clf_svm.cv_results_[\"params\"])):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params), file=svm_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Detailed classifciaiton reprot:\")\n",
    "print()\n",
    "print(\"the model is trained on the full development set.\")\n",
    "print(\"the scores are computed on the full developement set\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf_svm.predict(X_test)\n",
    "print(classification_report(y_true, y_pred), file=svm_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "svm_tf_idf_icf.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### end gridsearchCV for SVM classifer #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### gridsearchCV for naive_bayes classifer #######\n",
    "\n",
    "rb_idf_icf_tf = open('tf_idf_icf_rb.txt', 'w')\n",
    "\n",
    "\n",
    "parameters = [{\"alpha\": [00.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 6, 7, 8, 9, 10, 20, 40, 60, 80, 100, 150, 200, 300, 400, 500], \"fit_prior\": [\"True\", \"False\"]}]\n",
    "# scores = [\"precision\", \"recall_macro\"]\n",
    "#scores = [\"accuracy\"]\n",
    "scores = make_scorer(accuracy_score)\n",
    "\n",
    "print(\"# tuning hypter parmeter for %s\" % scores)\n",
    "print()\n",
    "\n",
    "clf_rb = GridSearchCV(rb, parameters, scoring=scores, verbose=3, n_jobs=6, cv=5)\n",
    "clf_rb.fit(X_train, y_train)\n",
    "\n",
    "print(\"best parameters st found on devleopment set:\")\n",
    "print()\n",
    "print(clf_rb.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "means = clf_rb.cv_results_[\"mean_test_score\"]\n",
    "stds = clf_rb.cv_results_[\"std_test_score\"]\n",
    "\n",
    "for mean, std, params in tqdm(zip(means, stds, clf_rb.cv_results_[\"params\"])):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params), file=rb_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Detailed classifciaiton reprot:\")\n",
    "print()\n",
    "print(\"the model is trained on the full development set.\")\n",
    "print(\"the scores are computed on the full developement set\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf_rb.predict(X_test)\n",
    "print(classification_report(y_true, y_pred), file=rb_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "svm_tf_idf_icf.close()\n",
    "\n",
    "\n",
    "###### end gridsearchCV for naive_bayes classifer #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### gridsearchCV for ramdon forest classifer #######\n",
    "\n",
    "rf_idf_icf_tf = open('tf_idf_icf_rf.txt', 'w')\n",
    "\n",
    "rf = RandomForestClassifier(random_state=2017)\n",
    "parameters = [{\"n_estimators\": [4, 6, 8, 10, 15, 20, 40, 60, 80, 100, 150, 200, 300, 400, 500, 600, 800, 1000], \"max_features\": ['sqrt', 'log2'], \"max_depth\": [4, 6, 8, 10, 12, 16, 18, 20, 22, 24, 26, 28, 30], 'criterion': ['gini', 'entropy', 'log_loss']}]\n",
    "scores = make_scorer(accuracy_score)\n",
    "\n",
    "print(\"# tuning hypter parmeter for %s\" % scores)\n",
    "print()\n",
    "\n",
    "clf_rf = GridSearchCV(rf, parameters, scoring=scores, verbose=3, n_jobs=6, cv=5)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"best parameters st found on devleopment set:\")\n",
    "print()\n",
    "print(clf_rf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "means = clf_rf.cv_results_[\"mean_test_score\"]\n",
    "stds = clf_rf.cv_results_[\"std_test_score\"]\n",
    "\n",
    "for mean, std, params in tqdm(zip(means, stds, clf_rf.cv_results_[\"params\"])):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params), file=rf_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Detailed classifciaiton reprot:\")\n",
    "print()\n",
    "print(\"the model is trained on the full development set.\")\n",
    "print(\"the scores are computed on the full developement set\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf_rb.predict(X_test)\n",
    "print(classification_report(y_true, y_pred), file=rf_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "rf_tf_idf_icf.close()\n",
    "\n",
    "\n",
    "\n",
    "##### end gridsearchCV for ramdon forest classifer #######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### gridsearchCV for Logistic regression classifer #######\n",
    "\n",
    "\n",
    "lr_tf_idf_icf = open('tf_idf_icf_lr.txt', 'w')\n",
    "\n",
    "lr = LogisticRegression(multi_class='multinomial', class_weight='balanced', random_state=2017)\n",
    "\n",
    "c_range = [0.001, 0.01, 0.1, 1, 10, 100, 200]\n",
    "max_i = [20, 50, 100, 200, 500, 1000]\n",
    "parameters = [{\"penalty\": [\"l1\"], \"C\": c_range, \"solver\": [\"saga\"], 'max_iter': max_i},\n",
    "              {\"penalty\": [\"l2\"], \"C\": c_range, \"solver\": [\"saga\"], 'max_iter': max_i},\n",
    "              {\"penalty\": [\"elasticnet\"], \"C\": c_range, \"solver\": [\"saga\"], 'max_iter': max_i, 'l1_ratio': [0.5]}]\n",
    "\n",
    "scores = make_scorer(accuracy_score)\n",
    "\n",
    "print(\"# tuning hypter parmeter for %s\" % scores)\n",
    "print()\n",
    "\n",
    "clf_lr = GridSearchCV(lr, parameters, scoring=scores, verbose=3, n_jobs=6, cv=5)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"best parameters st found on devleopment set:\")\n",
    "print()\n",
    "print(clf_lr.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "means = clf_lr.cv_results_[\"mean_test_score\"]\n",
    "stds = clf_lr.cv_results_[\"std_test_score\"]\n",
    "\n",
    "for mean, std, params in tqdm(zip(means, stds, clf_lr.cv_results_[\"params\"])):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params), file=lr_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Detailed classifciaiton reprot:\")\n",
    "print()\n",
    "print(\"the model is trained on the full development set.\")\n",
    "print(\"the scores are computed on the full developement set\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf_lr.predict(X_test)\n",
    "print(classification_report(y_true, y_pred), file=lr_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "lr_tf_idf_icf.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### end gridsearchCV for Logistic regression classifer #######\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##### gridsearchCV for XGBoost classifer #######\n",
    "\n",
    "\n",
    "xgb_tf_idf_icf = open('tf_idf_icf_xgb.txt', 'w')\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(random_state=2017)\n",
    "parameters = [{\"max_depth\":[2, 4, 8, 12, 16, 18, 22, 26], \"gamma\":[0.1, 1, 2, 4, 8, 12, 16, 20, 24], \"colsample_bytree\": [0.1, 0.2, 0.4, 0.6],\n",
    "               \"min_child_weight\":[1, 2, 4, 8, 10], \"subsample\":[0.1, 0.5, 0.7, 1]}]\n",
    "# scores = [\"precision\", \"recall_macro\"]\n",
    "#scores = [\"accuracy\"]\n",
    "scores = make_scorer(accuracy_score)\n",
    "\n",
    "print(\"# tuning hypter parmeter for %s\" % scores)\n",
    "print()\n",
    "\n",
    "clf_xgb = GridSearchCV(xgb, parameters, scoring=scores, verbose=3, n_jobs=6, cv=3)\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"best parameters st found on devleopment set:\")\n",
    "print()\n",
    "print(clf_xgb.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "means = clf_xgb.cv_results_[\"mean_test_score\"]\n",
    "stds = clf_xgb.cv_results_[\"std_test_score\"]\n",
    "\n",
    "for mean, std, params in tqdm(zip(means, stds, clf_xgb.cv_results_[\"params\"])):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params), file=xgb_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Detailed classifciaiton reprot:\")\n",
    "print()\n",
    "print(\"the model is trained on the full development set.\")\n",
    "print(\"the scores are computed on the full developement set\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf_xgb.predict(X_test)\n",
    "print(classification_report(y_true, y_pred), file=xgb_tf_idf_icf)\n",
    "\n",
    "print()\n",
    "\n",
    "xgb_tf_idf_icf.close()\n",
    "\n",
    "\n",
    "##### end gridsearchCV for XGBoost classifer #######\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
